{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pycharm-66c53067",
   "display_name": "PyCharm (InferenceSystem)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从本篇开始，整个机器学习系列还剩下最后三篇涉及导概率模型的文章，分别是EM算法、CRF条件随机场和HMM隐马尔科夫模型。本文主要讲解一下EM（Expection maximization），即期望最大化算法。EM算法是一种用于包含隐变量概率模型参数的极大似然估计方法，所以本文从极大似然方法说起，然后推广到EM算法。\n",
    "## 极大似然估计\n",
    "\n",
    "###    统计学专业的朋友对于极大似然估计一定是很熟悉了。极大似然是一种统计参数估计方法，对于某个随机样本满足某种概率分布，但其中的统计参数未知，我们通过若干次试验结果来估计参数的值的方法。\n",
    "###    举个例子来说。比如说我们想了解一下某校学生的身高分布。我们先假设该校学生身高服从一个正态分布$N\\left(\\mu, \\sigma^{2}\\right)$，其中的分布参数$\\mu$ 和 $\\sigma^{2}$未知。全校大几万的学生，我们要一个个去实测肯定不现实。所以我们决定用统计抽样的方法，随机选取100名学生来看一下身高。\n",
    "###    要通过这100人的身高来估算全校学生的身高，我们需要明确下面几个问题。第一个就是抽到这100人的概率是多少，因为每个人的选取都是独立的，所以选到这100人的概率可以表示为单个概率的乘积：\n",
    "# $L(\\theta)=L\\left(x_{1}, x_{2}, \\ldots, x_{n} ; \\theta\\right)=\\prod_{i=1}^{n} p\\left(x_{i} \\mid \\theta\\right)$\n",
    "##   上式即为似然函数。通常为了计算方便，我们会对似然函数取对数：\n",
    "# $H(\\theta)=\\ln L(\\theta)=\\ln \\prod_{i=1}^{n} p\\left(x_{i} \\mid \\theta\\right)=\\sum_{i=1}^{n} \\ln p\\left(x_{i} \\mid \\theta\\right)$   \n",
    "###    第二个问题在于我们要解释一下为什么能够刚好抽到这100人。所以按照极大似然估计的理论，在学校这么多人中，我们恰好抽到这100人而不是另外的100人，正是因为这100人出现的概率极大，即其对应的似然函数L( $\\theta)$ 极大：\n",
    "# $ \\hat{\\boldsymbol{\\theta}}=\\arg \\max L(\\theta) $\n",
    "\n",
    "###    第三个问题在于如何求解。这个好办，直接对求其参数的偏导数并令为0即可。\n",
    "    所以极大似然估计法可以看作由抽样结果对条件的反推，即已知某个参数能使这些样本出现的概率极大，我们就直接把该参数作为参数估计的真实值。\n",
    "## EM算法引入\n",
    "\n",
    "    上述基于全校学生身高服从一个分布的假设过于笼统，实际上该校男女生的身高分布是不一样的。其中男生的身高分布为，女生的身高分布为。现在我们估计该校学生的分布，就不能简单的一刀切了。\n",
    "    你可以说我们分别抽选50个男生和50个女生，对其分开进行估计。但大多数情况下，我们并不知道抽样得到的这个样本是来自于男生还是女生。如果说学生的身高的观测变量（Observable Variable），那么样本性别就是一种隐变量（Hidden Variable）。\n",
    "    在这种情况下，我们需要估计的问题包括两个：一个是这个样本是男的还是女的，二是男生和女生对应身高的正态分布参数分别是多少。这种情况下常规的极大似然估计就不太好使了，要估计男女身高分布，那必须先估计该学生是男还是女，反过来要估计该学生是男还是女，又得从身高来判断（男生身高相对较高，女生身高相对较矮）。但二者相互依赖，直接用极大似然估计没法算。\n",
    "    针对这种包含隐变量的参数估计问题，一般使用EM算法来进行求解。针对上述身高估计问题，EM算法的求解思想认为：既然两个问题相互依赖，肯定是一个动态求解过程。不如我们直接给定男生女生身高的分布初始值，根据初始值估计每个样本是男还是女的概率（E步），然后据此使用极大似然估计男女生的身高分布参数（M步），之后动态迭代调整直到满足终止条件为止。\n",
    "## EM算法\n",
    "\n",
    "    所以EM算法的应用场景就是要解决包含隐变量的概率模型参数估计问题。给定观测变量数据，隐变量数据，联合概率分布以及关于隐变量的条件分布，使用EM算法对模型参数进行估计流程如下：\n",
    "初始化模型参数，进行迭代：\n",
    "### E步：记为第次迭代参数的估计值，在第次迭代的E步，计算函数：\n",
    "# $Q\\left(\\theta, \\theta^{(l)}\\right)=E_{z}\\left[\\log P(Y, Z \\mid \\theta)\\left|Y, \\theta^{(\\prime)}\\right|=\\sum_{z} \\log P(Y, Z \\mid \\theta) P\\left(Z \\mid Y, \\theta^{0}\\right)\\right.$\n",
    "其中为给定观测数据和当前参数估计下隐变量数据的条件概率分布；\n",
    "### M步：求使Q函数最大化的参数 $\\theta$，确定第 $i+1$ 次迭代的参数估计值 \n",
    "# $ \\theta^{\\left(i+1\\right)} = \\arg \\max _{\\theta} Q\\left(\\theta, \\theta^{(0)}\\right)$\n",
    "重复迭代E步和M步直至收敛。\n",
    "    由EM算法过程我们可以看到，其关键在于E步要确定函数，E步在固定模型参数的情况下来估计隐变量分布，而M步则是固定隐变量来估计模型参数。二者交互进行，直至满足算法收敛条件。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://mp.weixin.qq.com/s?__biz=MzI4ODY2NjYzMQ==&mid=2247487755&idx=1&sn=93dc8dbf91b6516da1c23a5a7de616ed&chksm=ec3bb463db4c3d7535e67e03e1c4559c3d6779e54f655ad2355b18bc07cdfcb52a7d2a89f109&mpshare=1&scene=23&srcid=0728KqHvR9nDfqJvXBo590kW&sharer_sharetime=1595889589167&sharer_shareid=bf762a64dcd27edf3eb699de460c04fa%23rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $L(\\theta)=L\\left(x_{1}, x_{2}, \\ldots, x_{n} ; \\theta\\right)=\\prod_{i=1}^{n} p\\left(x_{i} \\mid \\theta\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logarithm(data,thetas):\n",
    "    # 对数似然\n",
    "    return np.array([np.sum(data * np.log(theta), axis=1) for theta in thetas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implicit_variables(logarithm):\n",
    "    # 似然\n",
    "    likelihood = np.exp(logarithm)\n",
    "    # 求隐变量分布\n",
    "    return likelihood/likelihood.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def EM(data, thetas, max_iter=50, eps=1e-3):\n",
    "    '''\n",
    "    data：观测数据\n",
    "    thetas：估计参数\n",
    "    max_iter：最大迭代次数\n",
    "    eps：收敛阈值\n",
    "    '''\n",
    "    # 初始化似然函数值\n",
    "    ll_old = -np.infty\n",
    "    for i in range(max_iter):\n",
    "        ### E步：求隐变量分布\n",
    "        # 对数似然\n",
    "        logarithm_ = logarithm(data,thetas)\n",
    "        # 求隐变量分布\n",
    "        ws = implicit_variables(logarithm_)\n",
    "        # 概率加权\n",
    "        vs = np.array([w[:, None] * data for w in ws])\n",
    "        ### M步：更新参数值\n",
    "        thetas = np.array([v.sum(0)/v.sum() for v in vs])\n",
    "        # 更新似然函数\n",
    "        ll_new = np.sum([w*l for w, l in zip(ws, logarithm_)])\n",
    "\n",
    "\n",
    "        print(\"Iteration: %d\" % (i+1))\n",
    "        print(\"theta_B = %.2f, theta_C = %.2f, ll = %.2f\" \n",
    "               % (thetas[0,0], thetas[1,0], ll_new))\n",
    "\n",
    "\n",
    "        # 满足迭代条件即退出迭代\n",
    "        if np.abs(ll_new - ll_old) < eps:\n",
    "            break\n",
    "        ll_old = ll_new\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.44914893 0.80498552 0.73346716 0.35215613 0.64721512]\n [0.55085107 0.19501448 0.26653284 0.64784387 0.35278488]]\nIteration: 1\ntheta_B = 0.71, theta_C = 0.58, ll = -32.69\n[[0.29581932 0.81151045 0.70642201 0.19014454 0.57353393]\n [0.70418068 0.18848955 0.29357799 0.80985546 0.42646607]]\nIteration: 2\ntheta_B = 0.75, theta_C = 0.57, ll = -31.26\n[[0.21759232 0.86984852 0.75115408 0.11159059 0.57686907]\n [0.78240768 0.13015148 0.24884592 0.88840941 0.42313093]]\nIteration: 3\ntheta_B = 0.77, theta_C = 0.55, ll = -30.76\n[[0.16170261 0.91290493 0.79426368 0.06633343 0.58710461]\n [0.83829739 0.08709507 0.20573632 0.93366657 0.41289539]]\nIteration: 4\ntheta_B = 0.78, theta_C = 0.53, ll = -30.33\n[[0.12902034 0.93537835 0.82155069 0.04499518 0.59420506]\n [0.87097966 0.06462165 0.17844931 0.95500482 0.40579494]]\nIteration: 5\ntheta_B = 0.79, theta_C = 0.53, ll = -30.07\n[[0.11354215 0.94527968 0.83523177 0.03622405 0.59798906]\n [0.88645785 0.05472032 0.16476823 0.96377595 0.40201094]]\nIteration: 6\ntheta_B = 0.79, theta_C = 0.52, ll = -29.95\n[[0.10708809 0.94933575 0.8412686  0.03280939 0.59985308]\n [0.89291191 0.05066425 0.1587314  0.96719061 0.40014692]]\nIteration: 7\ntheta_B = 0.80, theta_C = 0.52, ll = -29.90\n[[0.10455728 0.95095406 0.84378118 0.0315032  0.60074317]\n [0.89544272 0.04904594 0.15621882 0.9684968  0.39925683]]\nIteration: 8\ntheta_B = 0.80, theta_C = 0.52, ll = -29.88\n[[0.10359135 0.95159456 0.84480318 0.03100653 0.60115794]\n [0.89640865 0.04840544 0.15519682 0.96899347 0.39884206]]\nIteration: 9\ntheta_B = 0.80, theta_C = 0.52, ll = -29.87\n[[0.10322699 0.95184768 0.8452156  0.03081812 0.60134719]\n [0.89677301 0.04815232 0.1547844  0.96918188 0.39865281]]\nIteration: 10\ntheta_B = 0.80, theta_C = 0.52, ll = -29.87\n[[0.10309029 0.95194776 0.84538168 0.03074671 0.60143209]\n [0.89690971 0.04805224 0.15461832 0.96925329 0.39856791]]\nIteration: 11\ntheta_B = 0.80, theta_C = 0.52, ll = -29.87\n[[0.10303914 0.95198739 0.84544855 0.03071966 0.60146965]\n [0.89696086 0.04801261 0.15455145 0.96928034 0.39853035]]\nIteration: 12\ntheta_B = 0.80, theta_C = 0.52, ll = -29.87\n"
    }
   ],
   "source": [
    "# 观测数据，5次独立试验，每次试验10次抛掷的正反次数\n",
    "# 比如第一次试验为5次正面5次反面\n",
    "observed_data = np.array([(5,5), (9,1), (8,2), (4,6), (7,3)])\n",
    "# 初始化参数值，即硬币B的正面概率为0.6，硬币C的正面概率为0.5\n",
    "thetas = np.array([[0.6, 0.4], [0.5, 0.5]])\n",
    "eps = 0.01\n",
    "max_iter = 50\n",
    "thetas = EM(observed_data, thetas, max_iter=100, eps=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $L(\\theta)=L\\left(x_{1}, x_{2}, \\ldots, x_{n} ; \\theta\\right)=\\prod_{i=1}^{n} p\\left(x_{i} \\mid \\theta\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.6, 0.4],\n       [0.5, 0.5]])"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}