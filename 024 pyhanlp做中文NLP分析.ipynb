{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词\n",
    "\n",
    "pyhanlp可以自定义多种分词规则和模型，也可以加入自定义词典，经测试，默认的分词方法效果就不错，而且兼备词性标注以及命名实体识别，可以识别人名、地名、机构名等信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下雨天 n\n",
      "地面 n\n",
      "积水 n\n"
     ]
    }
   ],
   "source": [
    "#! pip install pyhanlp\n",
    "from pyhanlp import *\n",
    "sentence = \"下雨天地面积水\"\n",
    "\n",
    "# 返回一个list，每个list是一个分词后的Term对象，可以获取word属性和nature属性，分别对应的是词和词性\n",
    "terms = HanLP.segment(sentence )  \n",
    "for term in terms:\n",
    "\tprint(term.word,term.nature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关键词提取与自动摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[水资源, 陈明忠]\n"
     ]
    }
   ],
   "source": [
    "from pyhanlp import *\n",
    "\n",
    "document = \"水利部水资源司司长陈明忠9月29日在国务院新闻办举行的新闻发布会上透露，\" \\\n",
    "           \"根据刚刚完成了水资源管理制度的考核，有部分省接近了红线的指标，\" \\\n",
    "           \"有部分省超过红线的指标。对一些超过红线的地方，陈明忠表示，对一些取用水项目进行区域的限批，\" \\\n",
    "           \"严格地进行水资源论证和取水许可的批准。\"\n",
    "\n",
    "# 提取document的两个关键词\n",
    "print(HanLP.extractKeyword(document, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取ducument中的3个关键句作为摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[严格地进行水资源论证和取水许可的批准, 水利部水资源司司长陈明忠9月29日在国务院新闻办举行的新闻发布会上透露, 有部分省超过红线的指标]\n"
     ]
    }
   ],
   "source": [
    "print(HanLP.extractSummary(document, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 依存句法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t徐先生\t徐先生\tnh\tnr\t_\t4\t主谓关系\t_\t_\n",
      "2\t还\t还\td\td\t_\t4\t状中结构\t_\t_\n",
      "3\t具体\t具体\ta\tad\t_\t4\t状中结构\t_\t_\n",
      "4\t帮助\t帮助\tv\tv\t_\t0\t核心关系\t_\t_\n",
      "5\t他\t他\tr\tr\t_\t4\t兼语\t_\t_\n",
      "6\t确定\t确定\tv\tv\t_\t4\t动宾关系\t_\t_\n",
      "7\t了\t了\tu\tu\t_\t6\t右附加关系\t_\t_\n",
      "8\t把\t把\tp\tp\t_\t15\t状中结构\t_\t_\n",
      "9\t画\t画\tv\tv\t_\t8\t介宾关系\t_\t_\n",
      "10\t雄鹰\t雄鹰\tn\tn\t_\t9\t动宾关系\t_\t_\n",
      "11\t、\t、\twp\tw\t_\t12\t标点符号\t_\t_\n",
      "12\t松鼠\t松鼠\tn\tn\t_\t10\t并列关系\t_\t_\n",
      "13\t和\t和\tc\tc\t_\t14\t左附加关系\t_\t_\n",
      "14\t麻雀\t麻雀\tn\tn\t_\t10\t并列关系\t_\t_\n",
      "15\t作为\t作为\tv\tv\t_\t6\t动宾关系\t_\t_\n",
      "16\t主攻\t主攻\tv\tvn\t_\t17\t定中关系\t_\t_\n",
      "17\t目标\t目标\tn\tn\t_\t15\t动宾关系\t_\t_\n",
      "18\t。\t。\twp\tw\t_\t4\t标点符号\t_\t_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyhanlp import *\n",
    "print(HanLP.parseDependency(\"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 共性分析\n",
    "共性 是指 文本中词语共同出现的情况。\n",
    "一阶共性分析也就是统计词频，二阶分析和三阶分析主要用来发现短语。\n",
    "调用hanlp的共性分析模块，可以发现2个词或者3个词的出现次数（tf）、互信息（mi），左熵（le）、右熵（re）以及score。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 一阶共性分析，也就是词频统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一阶共性分析，也就是词频统计\n",
      "信息=1\n",
      "先进=1\n",
      "图形图像=1\n",
      "处理=2\n",
      "技术=1\n",
      "方面=1\n",
      "比较=1\n",
      "目前=1\n",
      "算法=2\n",
      "视频=1\n",
      "计算机=1\n",
      "音视频=1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyhanlp import * \n",
    "# 共性分析\n",
    "Occurrence = JClass(\"com.hankcs.hanlp.corpus.occurrence.Occurrence\")\n",
    "PairFrequency = JClass(\"com.hankcs.hanlp.corpus.occurrence.PairFrequency\")\n",
    "TermFrequency = JClass(\"com.hankcs.hanlp.corpus.occurrence.TermFrequency\")\n",
    "TriaFrequency = JClass(\"com.hankcs.hanlp.corpus.occurrence.TriaFrequency\")\n",
    "\n",
    "occurrence = Occurrence()\n",
    "occurrence.addAll(\"在计算机音视频和图形图像技术等二维信息算法处理方面目前比较先进的视频处理算法\")\n",
    "occurrence.compute()\n",
    "\n",
    "unigram = occurrence.getUniGram()\n",
    "for entry in unigram.iterator():\n",
    "    term_frequency = entry.getValue()\n",
    "    print(term_frequency)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三阶共性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "二阶共性分析\n",
      "信息→算法= tf=1 mi=8.856243954648566 le=0.0 re=0.0 score=0.8178260731171406\n",
      "先进→视频= tf=1 mi=6.594180024229758 le=0.0 re=0.0 score=0.6089367436420529\n",
      "图形图像→技术= tf=1 mi=20.46090157247892 le=0.0 re=0.0 score=1.8894532344802353\n",
      "处理→方面= tf=1 mi=4.04319404601706 le=0.0 re=0.0 score=0.37336703081322803\n",
      "处理→算法= tf=1 mi=9.247593120777918 le=0.0 re=0.0 score=0.8539650450551528\n",
      "技术→信息= tf=1 mi=4.012478779454232 le=0.0 re=0.0 score=0.3705306426145223\n",
      "方面→目前= tf=1 mi=12.825210015738996 le=0.0 re=0.0 score=1.184338552301167\n",
      "比较→先进= tf=1 mi=6.050081533887511 le=0.0 re=0.0 score=0.5586922004672908\n",
      "目前→比较= tf=1 mi=13.377862072309142 le=0.0 re=0.0 score=1.2353729709033823\n",
      "算法→处理= tf=1 mi=9.247593120777918 le=0.0 re=0.0 score=0.8539650450551528\n",
      "视频→处理= tf=1 mi=5.139944592929454 le=0.0 re=0.0 score=0.4746459925902054\n",
      "计算机→音视频= tf=1 mi=20.46090157247892 le=0.0 re=0.0 score=1.8894532344802353\n",
      "音视频→图形图像= tf=1 mi=20.46090157247892 le=0.0 re=0.0 score=1.8894532344802353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bigram = occurrence.getBiGram()\n",
    "for entry in bigram.iterator():\n",
    "    pair_frequency = entry.getValue()\n",
    "    if pair_frequency.isRight():\n",
    "        print(pair_frequency)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# '三阶共性分析'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三阶共性分析\n",
      "信息→算法→处理= tf=1 mi=0.0 le=0.0 re=0.0\n",
      "先进→视频→处理= tf=1 mi=0.0 le=0.0 re=0.0\n",
      "图形图像→技术→信息= tf=1 mi=0.0 le=0.0 re=0.0\n",
      "处理→方面→目前= tf=1 mi=0.0 le=0.0 re=0.0\n",
      "技术→信息→算法= tf=1 mi=0.0 le=0.0 re=0.0\n",
      "方面→目前→比较= tf=1 mi=0.0 le=0.0 re=0.0\n",
      "比较→先进→视频= tf=1 mi=0.0 le=0.0 re=0.0\n",
      "目前→比较→先进= tf=1 mi=0.0 le=0.0 re=0.0\n",
      "算法→处理→方面= tf=1 mi=0.0 le=0.0 re=0.0\n",
      "视频→处理→算法= tf=1 mi=0.0 le=0.0 re=0.0\n",
      "计算机→音视频→图形图像= tf=1 mi=0.0 le=0.0 re=0.0\n",
      "音视频→图形图像→技术= tf=1 mi=0.0 le=0.0 re=0.0\n"
     ]
    }
   ],
   "source": [
    "trigram = occurrence.getTriGram()\n",
    "for entry in trigram.iterator():\n",
    "    tria_frequency = entry.getValue()\n",
    "    if tria_frequency.isRight():\n",
    "        print(tria_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 短语提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[图形图像技术, 计算机音视频, 音视频图形图像, 处理算法, 算法处理, 信息算法, 先进视频, 比较先进, 视频处理, 方面比较]\n"
     ]
    }
   ],
   "source": [
    "text = \"在计算机音视频和图形图像技术等二维信息算法处理方面目前比较先进的视频处理算法\"\n",
    "phraseList = HanLP.extractPhrase(text, 10)\n",
    "print(phraseList);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tests.test_utility'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-28a50789ee0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyhanlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSafeJClass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_utility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mensure_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSafeJClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'com.hankcs.hanlp.classification.classifiers.NaiveBayesClassifier'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tests.test_utility'"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# Author：hankcs\n",
    "# Date: 2018-05-23 17:26\n",
    "import os\n",
    "from pyhanlp import SafeJClass\n",
    "from tests.test_utility import ensure_data\n",
    "\n",
    "NaiveBayesClassifier = SafeJClass('com.hankcs.hanlp.classification.classifiers.NaiveBayesClassifier')\n",
    "IOUtil = SafeJClass('com.hankcs.hanlp.corpus.io.IOUtil')\n",
    "sogou_corpus_path = ensure_data('搜狗文本分类语料库迷你版',\n",
    "                                'http://file.hankcs.com/corpus/sogou-text-classification-corpus-mini.zip')\n",
    "\n",
    "\n",
    "def train_or_load_classifier():\n",
    "    model_path = sogou_corpus_path + '.ser'\n",
    "    if os.path.isfile(model_path):\n",
    "        return NaiveBayesClassifier(IOUtil.readObjectFrom(model_path))\n",
    "    classifier = NaiveBayesClassifier()\n",
    "    classifier.train(sogou_corpus_path)\n",
    "    model = classifier.getModel()\n",
    "    IOUtil.saveObjectTo(model, model_path)\n",
    "    return NaiveBayesClassifier(model)\n",
    "\n",
    "\n",
    "def predict(classifier, text):\n",
    "    print(\"《%16s》\\t属于分类\\t【%s】\" % (text, classifier.classify(text)))\n",
    "    # 如需获取离散型随机变量的分布，请使用predict接口\n",
    "    # print(\"《%16s》\\t属于分类\\t【%s】\" % (text, classifier.predict(text)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    classifier = train_or_load_classifier()\n",
    "    predict(classifier, \"C罗获2018环球足球奖最佳球员 德尚荣膺最佳教练\")\n",
    "    predict(classifier, \"英国造航母耗时8年仍未服役 被中国速度远远甩在身后\")\n",
    "    predict(classifier, \"研究生考录模式亟待进一步专业化\")\n",
    "    predict(classifier, \"如果真想用食物解压,建议可以食用燕麦\")\n",
    "predict(classifier, \"通用及其部分竞争对手目前正在考虑解决库存问题\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
