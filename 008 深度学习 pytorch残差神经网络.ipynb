{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "No module named 'torchsummary'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e00784d17ca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "def conv_dw(inp, oup, stride):\n",
    "    # 深度可分离卷积层\n",
    "    return nn.Sequential(\n",
    "        # depthwise层：卷积计算层，计算输入inp卷积\n",
    "        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "        nn.BatchNorm2d(inp),\n",
    "        nn.ReLU6(inplace=True),\n",
    "        # pointwise层：卷积合并层，将计算的inp卷积与oup合并\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    # 标准卷积层\n",
    "    return nn.Sequential(\n",
    "        # 二维卷积，kernal_size(卷积核)=3*3, padding(边界填充数)=1\n",
    "        # stride(卷积窗口滑动步长)\n",
    "        # bias=False(卷积核只有权重系数，没有偏执系数，可减少训练参数)\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        # 归一化层，采取batchnorm归一化方法（不改变输入输出大小，只进行归一化操作）\n",
    "        nn.BatchNorm2d(oup),\n",
    "        # 激活函数\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    # 初始化手写识别网络模型\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # 标准3x3卷积层，名字为conv1，input=3，output=8，stride=1(输出特征宽高不变)\n",
    "        # 3x64x64 --> 8x64x64 （输入维度 --> 输出维度）\n",
    "        self.conv1 = conv_bn(3, 8, 1)\n",
    "        # 标准3x3卷积层，名字为conv2，input=8，output=16，stride=1\n",
    "        # 8x64x64 --> 16x64x64\n",
    "        self.conv2 = conv_bn(8, 16, 1)\n",
    "        # 深度可分离卷积层，名字为conv3，input=16，output=32，stride=1\n",
    "        # 16x64x64 --> 32x64x64\n",
    "        self.conv3 = conv_dw(16, 32, 1)\n",
    "        # 深度可分离卷积层，名字为conv4，input=32，output=32，stride=2(此时输出特征宽高减半)\n",
    "        # 32x64x64 --> 32x32x32\n",
    "        self.conv4 = conv_dw(32, 32, 2)\n",
    "        # 深度可分离卷积层，名字为conv5，input=32，output=64，stride=1\n",
    "        # 32x32x32 --> 64x32x32\n",
    "        self.conv5 = conv_dw(32, 64, 1)\n",
    "        # 深度可分离卷积层，名字为conv6，input=64，output=64，stride=2(此时输出特征宽高减半)\n",
    "        # 64x32x32 --> 64x16x16  下面依此类推\n",
    "        self.conv6 = conv_dw(64, 64, 2)  # 64x16x16\n",
    "        self.conv7 = conv_dw(64, 128, 1)  # 128x16x16\n",
    "        self.conv8 = conv_dw(128, 128, 1)  # 128x16x16\n",
    "        self.conv9 = conv_dw(128, 128, 1)  # 128x16x16\n",
    "        self.conv10 = conv_dw(128, 128, 1)  # 128x16x16\n",
    "        self.conv11 = conv_dw(128, 128, 1)  # 128x16x16\n",
    "        self.conv12 = conv_dw(128, 256, 2)  # 256x8x8\n",
    "        self.conv13 = conv_dw(256, 256, 1)  # 256x8x8\n",
    "        self.conv14 = conv_dw(256, 256, 1)  # 256x8x8\n",
    "        self.conv15 = conv_dw(256, 512, 2)  # 512x4x4\n",
    "        self.conv16 = conv_dw(512, 512, 1)  # 512x4x4\n",
    "        # 模型的分类器，分类器由如下网络层组合，输出为分类的类别\n",
    "        self.classifier = nn.Sequential(\n",
    "            # Linear为全连接层，将512*4*4的数据传入，输出1024\n",
    "            nn.Linear(512*4*4, 1024),\n",
    "            # Dropout层，功能：每个神经元都有0.2的概率被失效（即该神经元不起作用）\n",
    "            # Dropout层，作用：为了防止过拟合\n",
    "            nn.Dropout(0.2),\n",
    "            # ReLU 激活函数，inplace=True表示进行覆盖运算，即后面的计算值覆盖前面的\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 最后通过一个全连接层，输入1024数据，输出汉字字符个数（num_classes）\n",
    "            nn.Linear(1024, num_classes),\n",
    "        )\n",
    "        # 神经网络权重初始化（不同层不同初始化方法，看下面的注释）\n",
    "        self.weight_init()\n",
    "\n",
    "    # 网络模型的前向训练\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)  # x为输入数据（即图像数据），x1为经过卷积层conv1处理后的特征数据\n",
    "        x2 = self.conv2(x1)  # x1为上一层输出数据，传递到下层作为输入数据，经过卷积层conv2处理后输出数据为x2，以此类推\n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = self.conv4(x3)\n",
    "        x5 = self.conv5(x4)\n",
    "        x6 = self.conv6(x5)\n",
    "        x7 = self.conv7(x6)\n",
    "        x8 = self.conv8(x7)\n",
    "        x9 = self.conv9(x8)\n",
    "        # 将conv8卷积后的特征和conv9卷积后的特征累加作为特征，经激活函数relu处理后赋值给x9\n",
    "        # 目的：提高特征梯度，即使图像特征更明显\n",
    "        x9 = F.relu(x8 + x9)\n",
    "        x10 = self.conv10(x9)\n",
    "        x11 = self.conv11(x10)\n",
    "        x11 = F.relu(x10 + x11)\n",
    "        x12 = self.conv12(x11)\n",
    "        x13 = self.conv13(x12)\n",
    "        x14 = self.conv14(x13)\n",
    "        x14 = F.relu(x13 + x14)\n",
    "        x15 = self.conv15(x14)\n",
    "        x16 = self.conv16(x15)\n",
    "        # 修改x16数据tensor形状为x16.size(0)*?, 此时？由x16的数据量自己推算\n",
    "        # 如x16的size为3*3*2，则x = x16.view(x16.size(0), -1)后，x的size为3*6\n",
    "        x = x16.view(x16.size(0), -1)\n",
    "        # 经分类器输出类别tensor数据x\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def weight_init(self):\n",
    "        # 权重初始化，根据不同的层类别进行不同的初始化方法\n",
    "        for layer in self.modules():\n",
    "            self._layer_init(layer)\n",
    "\n",
    "    def _layer_init(self, m):\n",
    "        # 使用isinstance来判断m属于什么类型的网络层\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            # 二维卷积层，初始化方法为：均值为0，方差为np.sqrt(2. / n)的正态分布进行随机初始化\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            # batchnorm层的weight和bias为可学习变量，将weight初始化为1，bias为0\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            # 全连接层，weight初始化：均值为0，方差为0.01的正态分布进行随机初始化，bias初始化为0\n",
    "            m.weight.data.normal_(0, 0.01)\n",
    "            m.bias.data.zero_()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = ConvNet(3755).cuda()\n",
    "    summary(model, input_size=(3, 64, 64), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python35664bitdeeplearningconda49c51f4718784ba4ab06df616fe53864",
   "display_name": "Python 3.5.6 64-bit ('deeplearning': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}