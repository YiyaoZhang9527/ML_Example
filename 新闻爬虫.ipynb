{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 None\n1 人脸识别也成了灰色生意！谁在卖你的脸，又是谁在买？\n2 人脸识别也成了灰色生意！谁在卖你的脸，又是谁在买？\n3 []\n4 ['人脸识别也成了灰色生意谁在卖你的脸又是谁在买', '财经腾讯网']\n财经-腾讯网\nNone\nhttps://mat1.gtimg.com/www/icon/favicon2.ico\n{'https://inews.gtimg.com/newsapp_ls/0/12055930722_870492/0', 'https://mat1.gtimg.com/pingjs/ext2020/newom/build/static/images/new_logo.png', 'https://inews.gtimg.com/newsapp_ls/0/12055965485_870492/0', 'https://mat1.gtimg.com/www/icon/favicon2.ico', 'https://inews.gtimg.com/newsapp_ls/0/12056372414_870492/0'}\n"
    }
   ],
   "source": [
    "import newspaper\n",
    "from newspaper import Article\n",
    "\n",
    "#将文章下载到内存的基础\n",
    "article = Article(\"https://new.qq.com/ch/finance/\")\n",
    "article.download()\n",
    "article.parse()\n",
    "article.nlp()\n",
    "print(0,article.release_resources())\n",
    "# 输出全文\n",
    "print(1,article.text)\n",
    "\n",
    "# 输出文本摘要\n",
    "# 因为newspaper3k内置了NLP工具，这一步行之有效\n",
    "print(2,article.summary)\n",
    "\n",
    "# 输出作者名字\n",
    "print(3,article.authors)\n",
    "\n",
    "# 输出关键字列表\n",
    "print(4,article.keywords)\n",
    "\n",
    "#收集文章中其他有用元数据的其他函数\n",
    "print(article.title) # 给出标题\n",
    "print(article.publish_date) #给出文章发表的日期\n",
    "print(article.top_image) # 链接到文章的主要图像\n",
    "print(article.images) # 提供一组图像链接import newspaper\n",
    "# print(news.html)\n",
    "# print(news.authors)\n",
    "# print(news.top_image)\n",
    "# print(news.movies)\n",
    "# print(news.keywords)\n",
    "# print(news.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "from newspaper import Article\n",
    "from newspaper import Source\n",
    "import pandas as pd\n",
    "\n",
    "# 假设我们要从Gamespot（该网站讨论视频游戏）下载文章\n",
    "gamespot = newspaper.build(\"https://www.gamespot.com//news/\", memoize_articles = False) \n",
    "#我将memoize_articles设置为False，因为我不希望它缓存文章并将其保存到内存中，然后再运行。\n",
    "# 全新运行，每次运行时都基本上执行此脚本\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "for each_article in gamespot.articles:\n",
    "  \n",
    "  each_article.download()\n",
    "  each_article.parse()\n",
    "  each_article.nlp()\n",
    "  \n",
    "  temp_df = pd.DataFrame(columns = ['Title', 'Authors', 'Text',\n",
    "                                    'Summary', 'published_date', 'Source'])\n",
    "  \n",
    "  temp_df['Authors'] = each_article.authors\n",
    "  temp_df['Title'] = each_article.title\n",
    "  temp_df['Text'] = each_article.text\n",
    "  temp_df['Summary'] = each_article.summary\n",
    "  temp_df['published_date'] = each_article.publish_date\n",
    "  temp_df['Source'] = each_article.source_url\n",
    "  \n",
    "  final_df = final_df.append(temp_df, ignore_index = True)\n",
    "  \n",
    "#从这里可以将此Pandas数据框导出到csv文件\n",
    "final_df.to_csv('my_scraped_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n404-页面不存在\n"
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "url = 'http://news.ifeng.com/a/20180504/58107235_0.shtml'\n",
    "news = Article(url, language='zh')\n",
    "news .download()\n",
    "news .parse()\n",
    "print(news.text)\n",
    "print(news.title)\n",
    "# print(news.html)\n",
    "# print(news.authors)\n",
    "# print(news.top_image)\n",
    "# print(news.movies)\n",
    "# print(news.keywords)\n",
    "# print(news.summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "http://zhibo.sina.com.cn/api/zhibo/feed?            callback=jQuery0&page=1&page_size=1&zhibo_id=152            &tag_id=0&dire=f&dpc=1&pagesize=1&_=0%20Request%20Method:GET\n时间: 2020-07-09 00:07:43\n内容:   【17家券商6月净利翻倍，国君、建投大赚超10亿】截至8日晚7点，共有35家A股上市券商披露了6月月报。35家披露业绩数据的券商中，有12家券商营收超过10亿元，其中中信证券、国泰君安、广发证券、中信建投营收均超过20亿元，分别以25亿、23.57亿元、21.57亿元以及20.42亿元的收入暂居行业前四。（券商中国）\nhttp://zhibo.sina.com.cn/api/zhibo/feed?            callback=jQuery0&page=1&page_size=1&zhibo_id=152            &tag_id=0&dire=f&dpc=1&pagesize=1&_=0%20Request%20Method:GET\n此次刷新没有获得数据，等待5s后将继续刷新 0\nhttp://zhibo.sina.com.cn/api/zhibo/feed?            callback=jQuery0&page=1&page_size=1&zhibo_id=152            &tag_id=0&dire=f&dpc=1&pagesize=1&_=0%20Request%20Method:GET\n此次刷新没有获得数据，等待5s后将继续刷新 1\nhttp://zhibo.sina.com.cn/api/zhibo/feed?            callback=jQuery0&page=1&page_size=1&zhibo_id=152            &tag_id=0&dire=f&dpc=1&pagesize=1&_=0%20Request%20Method:GET\n此次刷新没有获得数据，等待5s后将继续刷新 2\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0745b6514b2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    149\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlastdateid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'此次刷新没有获得数据，等待5s后将继续刷新'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m                         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "'''\n",
    "version：2.0\n",
    "代码重新架构，把脚本封装为函数以方便处理数据\n",
    "增加了测试模块，以防止网络波动导致的代码运行中断\n",
    "'''\n",
    "lastdateid = '0'\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# 获取最新消息的地址\n",
    "base_url_new = 'http://zhibo.sina.com.cn/api/zhibo/feed?\\\n",
    "            callback=jQuery0&page=1&page_size=1&zhibo_id=152\\\n",
    "            &tag_id=0&dire=f&dpc=1&pagesize=1&_=0%20Request%20Method:GET'\n",
    "\n",
    "# 自定义js提取页\n",
    "'''\n",
    "输入请用str格式\n",
    "oldnum_str:此数字或者是以现在为计时之前的页数\n",
    "thispagesize:此次取的js数据页的数据数\n",
    "oldpagesize:原来的js数据页的数据数目\n",
    "'''\n",
    "'''\n",
    "\t#此url的数据中会存在thispagesize_str条信息，比最新信息早  oldnum_str* oldpagesize_str条\n",
    "#oldnum_str max 527999 \n",
    "'''\n",
    "\n",
    "\n",
    "def myurl(oldnum_str, thispagesize_str, oldpagesize_str):\n",
    "    my_url_a = r'http://zhibo.sina.com.cn/api/zhibo/feed?\\\n",
    "                    callback=jQuery0&page='\n",
    "    my_url_b = r'&page_size='\n",
    "    my_url_c = r'&zhibo_id=152&tag_id=0&dire=f&dpc=1&pagesize='\n",
    "    my_url_d = r'&_=0%20Request%20Method:GET'\n",
    "    my_url = my_url_a + oldnum_str + my_url_b + thispagesize_str + my_url_c + oldpagesize_str + my_url_d  # 获取往前的第n条消息\n",
    "    return my_url\n",
    "\n",
    "\n",
    "i = 1  # 本代码运行后得到的数据量标号\n",
    "n = 1  # 等待时间标号一个为5s\n",
    "\n",
    "\n",
    "# 从js数据包中获得json数据\n",
    "def get_json_str(base_url):\n",
    "    json_str = ''\n",
    "    try:\n",
    "        response = requests.get(base_url, timeout=5)\n",
    "        print(base_url)\n",
    "        html = response.text\n",
    "        html_cl = html[12:-14]\n",
    "        html_json = eval(html_cl)\n",
    "        json_str = json.dumps(html_json)\n",
    "    except ConnectionError:\n",
    "        print('get_json_str得到数据时网络连接有问题，未得到json数据')  # 此处数据穿送出去为了后期处理\n",
    "    except UnboundLocalError:\n",
    "        print('get_json_str您的电脑休眠导致连接中断')  # 此处数据穿送出去为了后期处理\n",
    "    except Exception as errorname1:\n",
    "        print('get_json_str未收录错误类型，请检查网络通断,错误位置：', errorname1)\n",
    "        # print(errorname1)# 此处数据穿送出去为了后期处理'''\n",
    "    finally:\n",
    "        return json_str\n",
    "\n",
    "\n",
    "'''\n",
    "json 数据解析函数 \n",
    "输入：json类型的str数据\n",
    "数出：以数据id作为key的字典\n",
    "'''\n",
    "\n",
    "\n",
    "def json_str_analysis(json_date):\n",
    "    date_dic = {}\n",
    "    try:\n",
    "        python_dic = json.loads(json_date)  # 先拆分为主字典\n",
    "\n",
    "        list_str = python_dic[\"result\"]['data']['feed']['list']  # 取数据列表\n",
    "        '''\n",
    "        重新整定字典\n",
    "        date_dic=\n",
    "            {数据在新浪数据库的id:{'id':         ,\n",
    "                                 'rich_text':    ,\n",
    "                                 'create_time':    ,\n",
    "                                 'tag':{}            ,\n",
    "                                 }}\n",
    "        '''\n",
    "\n",
    "        for list_dic in list_str:  # 对list的多组数据解析\n",
    "            need_option = ['id', 'rich_text', 'create_time', 'tag']  # 定义需求关键词\n",
    "            for listkey in list(list_dic.keys()):  # 字典在遍历的时候不能修改元素，此处先编程列表在变回字典\n",
    "                if listkey not in need_option:\n",
    "                    list_dic.pop(listkey)\n",
    "            date_dic[list_dic['id']] = list_dic\n",
    "    except ValueError:\n",
    "        print('json_str_analysis  json数据格式不对无法解析')\n",
    "    except Exception as errorname2:  # 将报错存储在errorname2中\n",
    "        print('json_str_analysis未知问题', errorname2)\n",
    "    finally:\n",
    "        return date_dic\n",
    "\n",
    "\n",
    "def list_dic_display(listdic):\n",
    "    create_timez_str = tag_id_str = type_str = rich_text_str = data_id_str = ''\n",
    "    # 下面为解析出来的数据\n",
    "    data_id_str = listdic['id']\n",
    "    rich_text_str = listdic['rich_text']\n",
    "    create_timez_str = listdic['create_time']\n",
    "    tag_str = listdic['tag']\n",
    "\n",
    "    # tag对象可能有多个，此处用for解析\n",
    "    tag_id_str = []\n",
    "    type_str = []\n",
    "    for tag_dic in tag_str:\n",
    "        # print(tagpp)\n",
    "        tag_id_str.append(tag_dic['id'])\n",
    "        type_str.append(tag_dic['name'])\n",
    "        # 有些段落有图片此处给出解析图片url的api\n",
    "        # multimedia_url=list_dic['multimedia']\n",
    "    # 此处将数据打印出来\n",
    "    #print('新浪数据库中id为', data_id_str, '的数据')\n",
    "    print('时间:', create_timez_str)\n",
    "    #print('id:', tag_id_str, '     类型:', type_str)\n",
    "    print('内容:', rich_text_str)\n",
    "\n",
    "\n",
    "\n",
    "#def post_url(listdic):\n",
    "    url = \"https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=49d89fd0-5cdb-4c30-85f8-52efb2eab63f\"\n",
    "    headers = {\"Content-Type\": \"application/json\",  \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36\"}\n",
    "    data = json.dumps({\"msgtype\": \"text\", \"text\": {\"content\": \"【时间】：\"+create_timez_str+\"，【内容】：\"+rich_text_str}})\n",
    "    requests.post(url=url, data=data, headers=headers)\n",
    "\n",
    "\n",
    "    return rich_text_str\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    while (True):\n",
    "        json_date = get_json_str(base_url_new)\n",
    "        # print('json_date:',json_date)\n",
    "        if json_date:\n",
    "            jsaoutput_dic = json_str_analysis(json_date)\n",
    "            # print('jsaoutput_dic:',jsaoutput_dic)\n",
    "            if jsaoutput_dic:\n",
    "                for dataid in jsaoutput_dic.keys():\n",
    "                    if (int(dataid)) <= (int(lastdateid)):\n",
    "                        print('此次刷新没有获得数据，等待5s后将继续刷新', n)\n",
    "                        time.sleep(5)\n",
    "                        n = n + 1\n",
    "                    else:\n",
    "                        try:\n",
    "                            richtextstr = list_dic_display(jsaoutput_dic[dataid])\n",
    "                            #post_url(jsaoutput_dic[dataid])\n",
    "                            i = i + 1\n",
    "                            n = 0\n",
    "                            lastdateid = dataid\n",
    "                            time.sleep(5)\n",
    "                        except ValueError:\n",
    "                            print('main处理数据错误')\n",
    "                        except Exception as errorname3:  # 将报错存储在 e 中\n",
    "                            print(errorname3)\n",
    "                            # print(output)\n",
    "            else:\n",
    "                print('jsa_flag为0，json_str_analysis解析数据失败')\n",
    "                print('等待5s后将继续刷新')\n",
    "                time.sleep(5)\n",
    "        else:\n",
    "            print('get_json_flag为0，get_json_str未能得到json数据')\n",
    "            print('等待5s后将继续刷新')\n",
    "            time.sleep(5)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pycharm-66c53067",
   "display_name": "PyCharm (InferenceSystem)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}