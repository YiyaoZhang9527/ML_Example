{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f3a2f0d38607>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x):\n",
    "    return (x**2)*torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.linspace(-1,1,100)\n",
    "plt.plot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss(X))\n",
    "plt.plot(np.arange(len(X)),np.gradient(loss(X)),c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ y = w * x + b + \\epsilon $\n",
    "# $ \\epsilon ~ N(0.01,1) $\n",
    "## 生成y数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(s,e,t):\n",
    "    init = np.linspace(s,e,t)\n",
    "    N = np.random.normal(loc=0.01,scale=1,size=t)\n",
    "    return init+N\n",
    "\n",
    "X = create_data(-1,1,100)\n",
    "plt.scatter(np.arange(X.size),X,c='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def is_GPU(tensor):\n",
    "    '''\n",
    "    函数名 : cpu 或者 gpu 自动推断函数\n",
    "    :param tensor: tensor结构数据\n",
    "    :return: 根据系统转cpu或者gpu结构\n",
    "    '''\n",
    "    if torch.cuda.is_available():\n",
    "        return tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "def yhat_function(X,theta,b): # 期望函数\n",
    "    '''\n",
    "    函数名 : yhot函数 / 模型的期望的y值的函数\n",
    "    :param X: 数据集\n",
    "    :param a: 待迭代参数\n",
    "    :param b: 偏置选项\n",
    "    :return: 模型的线性期望\n",
    "    '''\n",
    "    return is_GPU(X.mm(theta)+b)\n",
    "\n",
    "def initialize_theta(dims,b=None): # 初始化参数\n",
    "    '''\n",
    "    函数名 : 初始化参数函数\n",
    "    :param dims: 数据集的列数\n",
    "    :return: thate -> 初始化的参数 ， b -> 初始化偏置\n",
    "    '''\n",
    "    \n",
    "    theta = is_GPU(torch.zeros((dims, 1)))\n",
    "    if b == None :\n",
    "        b = 0\n",
    "    return theta, b\n",
    "\n",
    "\n",
    "def loss_function(y_hat,y,m): # 损失函数\n",
    "    '''\n",
    "    函数名 : 损失函数\n",
    "    :param y_hat: 模型当前拟合的y值\n",
    "    :param y: 真实的y值\n",
    "    :param m: 数据集的行数\n",
    "    :return: 损失值\n",
    "    '''\n",
    "    return is_GPU(((y_hat-y)**2).sum()/m)\n",
    "\n",
    "def partial_derivative_function(X,yhat,Y,m): # 求偏导数\n",
    "    '''\n",
    "    函数名 : 求偏导数函数\n",
    "    :param X: 数据集\n",
    "    :param yhat: y值的拟合过程\n",
    "    :param Y: 真实的y值\n",
    "    :param m: 数据集X的行数\n",
    "    :return: 偏导数\n",
    "    '''\n",
    "    dw = X.T.mm(yhat-Y)/m\n",
    "    db = (yhat-Y).sum()/m\n",
    "    return dw,db\n",
    "\n",
    "def liner_model(X_train,y_train,b=None\n",
    "                ,maxloop = 10000\n",
    "                ,alpha = 0.001\n",
    "                ,convergence_threshold = 10**(-8)\n",
    "                ,display=False,display_cycle=False):\n",
    "    '''\n",
    "    :param X_train: 训练集X\n",
    "    :param y_train: 训练标签 y\n",
    "    :param b:偏置项\n",
    "    :param maxloop: 最大迭代次数\n",
    "    :param alpha: 学习率\n",
    "    :param convergence_threshold: 收敛阈值\n",
    "    :param display:是否显示迭代过程与损失函数图像\n",
    "    :param display_cycle: 显示迭代过程的间隔，默认每1/10次显示\n",
    "    :return: lossdata -> 损失函数过程收集,loss ->最终损失值 , params —> 参数 , grads -> 梯度值 ,gradsdata -> 梯度迭代函数\n",
    "    '''\n",
    "\n",
    "    theta,b = initialize_theta(X_train.shape[1],b)\n",
    "    lossdata = [np.Inf]\n",
    "    m,n = X_train.shape\n",
    "    gradsdata=[]\n",
    "    for i in range(1,maxloop):\n",
    "        yhat = yhat_function(X_train,theta,b)\n",
    "        loss = loss_function(yhat,y_train,m)\n",
    "        dw,db = partial_derivative_function(X_train,yhat,y_train,m)\n",
    "        # 参数更新过程\n",
    "        theta += is_GPU(-alpha * dw)\n",
    "        b += is_GPU(-alpha * db)\n",
    "        # 保存参数\n",
    "        params = {'theta': theta,'b': b}\n",
    "        # 保存梯度\n",
    "        grads = {'dw': dw,'db': db}\n",
    "        \n",
    "        gradsdata.append([dw,db])\n",
    "        lossdata.append(loss)\n",
    "        if display:\n",
    "            cycle = display_cycle == False and maxloop/10 or display\n",
    "            if i % cycle == 0:\n",
    "                print('epoch %d loss %f' % (i, loss))\n",
    "        if lossdata[-2]-lossdata[-1] <= convergence_threshold:\n",
    "            break\n",
    "    if display:\n",
    "        plt.plot(lossdata)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    print(i)\n",
    "    return lossdata,loss, params, grads,gradsdata\n",
    "\n",
    "def predict(X, params):\n",
    "    '''\n",
    "    :param X:\n",
    "    :param params:\n",
    "    :return:\n",
    "    '''\n",
    "    w = params['theta']\n",
    "    b = params['b']\n",
    "    y_pred = X.mm(w) + b\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = torch.tensor(X)\n",
    "test_Y = torch.range(0,test_X.size()[0])\n",
    "liner_model(test_X,test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
