{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  tushare as ts\n",
    "# 获取代号为000300的股票价格\n",
    "cons=ts.get_apis()\n",
    "df1=ts.bar('000001', conn=cons, asset='INDEX', start_date='2018-01-01', end_date='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 对于获取的数据按日期进行升序排列，因为我们要通过历史的情况预测未来的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            code     open    close     high      low        vol        amount  \\\ndatetime                                                                        \n2018-01-02   1.0  3314.03  3348.33  3349.05  3314.03  2022788.0  2.277885e+11   \n2018-01-03   1.0  3347.74  3369.11  3379.92  3345.29  2138361.0  2.583665e+11   \n2018-01-04   1.0  3371.00  3385.71  3392.83  3365.30  2069552.0  2.430908e+11   \n2018-01-05   1.0  3386.46  3391.75  3402.07  3380.25  2130606.0  2.481878e+11   \n2018-01-08   1.0  3391.55  3409.48  3412.73  3384.56  2361651.0  2.862132e+11   \n2018-01-09   1.0  3406.11  3413.90  3417.23  3403.59  1914885.0  2.382500e+11   \n2018-01-10   1.0  3414.11  3421.83  3430.21  3398.84  2090949.0  2.545154e+11   \n2018-01-11   1.0  3415.58  3425.35  3426.48  3405.64  1738121.0  2.184141e+11   \n2018-01-12   1.0  3423.88  3428.94  3435.42  3417.98  1740634.0  2.159615e+11   \n2018-01-15   1.0  3428.95  3410.49  3442.50  3402.31  2320092.0  2.863627e+11   \n\n            p_change  \ndatetime              \n2018-01-02       NaN  \n2018-01-03      0.62  \n2018-01-04      0.49  \n2018-01-05      0.18  \n2018-01-08      0.52  \n2018-01-09      0.13  \n2018-01-10      0.23  \n2018-01-11      0.10  \n2018-01-12      0.10  \n2018-01-15     -0.54  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>code</th>\n      <th>open</th>\n      <th>close</th>\n      <th>high</th>\n      <th>low</th>\n      <th>vol</th>\n      <th>amount</th>\n      <th>p_change</th>\n    </tr>\n    <tr>\n      <th>datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-01-02</th>\n      <td>1.0</td>\n      <td>3314.03</td>\n      <td>3348.33</td>\n      <td>3349.05</td>\n      <td>3314.03</td>\n      <td>2022788.0</td>\n      <td>2.277885e+11</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2018-01-03</th>\n      <td>1.0</td>\n      <td>3347.74</td>\n      <td>3369.11</td>\n      <td>3379.92</td>\n      <td>3345.29</td>\n      <td>2138361.0</td>\n      <td>2.583665e+11</td>\n      <td>0.62</td>\n    </tr>\n    <tr>\n      <th>2018-01-04</th>\n      <td>1.0</td>\n      <td>3371.00</td>\n      <td>3385.71</td>\n      <td>3392.83</td>\n      <td>3365.30</td>\n      <td>2069552.0</td>\n      <td>2.430908e+11</td>\n      <td>0.49</td>\n    </tr>\n    <tr>\n      <th>2018-01-05</th>\n      <td>1.0</td>\n      <td>3386.46</td>\n      <td>3391.75</td>\n      <td>3402.07</td>\n      <td>3380.25</td>\n      <td>2130606.0</td>\n      <td>2.481878e+11</td>\n      <td>0.18</td>\n    </tr>\n    <tr>\n      <th>2018-01-08</th>\n      <td>1.0</td>\n      <td>3391.55</td>\n      <td>3409.48</td>\n      <td>3412.73</td>\n      <td>3384.56</td>\n      <td>2361651.0</td>\n      <td>2.862132e+11</td>\n      <td>0.52</td>\n    </tr>\n    <tr>\n      <th>2018-01-09</th>\n      <td>1.0</td>\n      <td>3406.11</td>\n      <td>3413.90</td>\n      <td>3417.23</td>\n      <td>3403.59</td>\n      <td>1914885.0</td>\n      <td>2.382500e+11</td>\n      <td>0.13</td>\n    </tr>\n    <tr>\n      <th>2018-01-10</th>\n      <td>1.0</td>\n      <td>3414.11</td>\n      <td>3421.83</td>\n      <td>3430.21</td>\n      <td>3398.84</td>\n      <td>2090949.0</td>\n      <td>2.545154e+11</td>\n      <td>0.23</td>\n    </tr>\n    <tr>\n      <th>2018-01-11</th>\n      <td>1.0</td>\n      <td>3415.58</td>\n      <td>3425.35</td>\n      <td>3426.48</td>\n      <td>3405.64</td>\n      <td>1738121.0</td>\n      <td>2.184141e+11</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>2018-01-12</th>\n      <td>1.0</td>\n      <td>3423.88</td>\n      <td>3428.94</td>\n      <td>3435.42</td>\n      <td>3417.98</td>\n      <td>1740634.0</td>\n      <td>2.159615e+11</td>\n      <td>0.10</td>\n    </tr>\n    <tr>\n      <th>2018-01-15</th>\n      <td>1.0</td>\n      <td>3428.95</td>\n      <td>3410.49</td>\n      <td>3442.50</td>\n      <td>3402.31</td>\n      <td>2320092.0</td>\n      <td>2.863627e+11</td>\n      <td>-0.54</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "df=df1.sort_index(ascending=True).astype(float)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.取开盘价，收盘价，最高价，最低价，交易量五个特征，并做标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df=df[[\"open\",\"close\",\"high\",\"low\",\"vol\"]]\n",
    "df=df.apply(lambda x:(x-min(x))/(max(x)-min(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import *\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "def SeriesGen(N):\n",
    "    x = torch.arange(1,N,0.01)\n",
    "    return torch.sin(x)\n",
    "'''\n",
    "def SeriesGen():\n",
    "    x = torch.FloatTensor(df['high'].to_numpy())\n",
    "    return x\n",
    " \n",
    "def trainDataGen(seq,k):\n",
    "    dat = list()\n",
    "    L = len(seq)\n",
    "    for i in range(L-k-1):\n",
    "        indat = seq[i:i+k]\n",
    "        outdat = seq[i+1:i+k+1]\n",
    "        dat.append((indat,outdat))\n",
    "    return dat\n",
    " \n",
    "def ToVariable(x):\n",
    "    tmp = torch.FloatTensor(x)\n",
    "    return Variable(tmp)\n",
    " \n",
    "y = SeriesGen()\n",
    "dat = trainDataGen(y.numpy(),10)\n",
    " \n",
    " \n",
    "class LSTMpred(nn.Module):\n",
    " \n",
    "    def __init__(self,input_size,hidden_dim):\n",
    "        super(LSTMpred,self).__init__()\n",
    "        self.input_dim = input_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size,hidden_dim)\n",
    "        self.hidden2out = nn.Linear(hidden_dim,1)\n",
    "        self.hidden = self.init_hidden()\n",
    " \n",
    "    def init_hidden(self):\n",
    "        return (Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    " \n",
    "    def forward(self,seq):\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            seq.view(len(seq), 1, -1), self.hidden)\n",
    "        outdat = self.hidden2out(lstm_out.view(len(seq),-1))\n",
    "        return outdat\n",
    " \n",
    " \n",
    "model = LSTMpred(1,6)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    " \n",
    "for epoch in range(10):\n",
    "    print(epoch)\n",
    "    for seq, outs in dat[:700]:\n",
    "        seq = ToVariable(seq)\n",
    "        outs = ToVariable(outs)\n",
    "        #outs = torch.from_numpy(np.array([outs]))\n",
    " \n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        model.hidden = model.init_hidden()\n",
    " \n",
    "        modout = model(seq)\n",
    " \n",
    "        loss = loss_function(modout, outs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "predDat = []\n",
    "for seq, trueVal in dat[700:]:\n",
    "    seq = ToVariable(seq)\n",
    "    trueVal = ToVariable(trueVal)\n",
    "    predDat.append(model(seq)[-1].data.numpy()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y.shape,len(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(y.numpy())\n",
    "plt.plot(list(range(len(predDat))), predDat) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "https://blog.csdn.net/hustchenze/article/details/78696771?ops_request_misc=&request_id=&biz_id=102&utm_term=pytorch%201.4%20lstm&0utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-5-78696771"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mp.weixin.qq.com/s?__biz=MzA5NDM4MjMxOQ==&mid=2447578969&idx=1&sn=1ae03db749b56b1d2a140e1369bd8dba&chksm=8458c6d9b32f4fcf2ade01c726fa734476a9e173eb1189cc3a4eef3e0d350a635ea986e8f35c&mpshare=1&scene=1&srcid=0212XZYGJ80skfO7hOjclOb7&pass_ticket=4Ny3sz5foHRLUkvLoDFapNEMyCtao9JWtiG8SlAqvSYQiUvIQsIROlNPJ5l3iFyN#rd\n",
    "https://blog.csdn.net/yangwohenmai1/article/details/84874197?utm_medium=distribute.pc_relevant.none-task-blog-baidujs-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import *\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "def SeriesGen(N):\n",
    "    x = torch.arange(1,N,0.01)\n",
    "    return torch.sin(x)\n",
    " \n",
    "def trainDataGen(seq,k):\n",
    "    dat = list()\n",
    "    L = len(seq)\n",
    "    for i in range(L-k-1):\n",
    "        indat = seq[i:i+k]\n",
    "        outdat = seq[i+1:i+k+1]\n",
    "        dat.append((indat,outdat))\n",
    "    return dat\n",
    " \n",
    "def ToVariable(x):\n",
    "    tmp = torch.FloatTensor(x)\n",
    "    return Variable(tmp)\n",
    " \n",
    "y = SeriesGen(10)\n",
    "dat = trainDataGen(y.numpy(),10)\n",
    " \n",
    " \n",
    "class LSTMpred(nn.Module):\n",
    " \n",
    "    def __init__(self,input_size,hidden_dim):\n",
    "        super(LSTMpred,self).__init__()\n",
    "        self.input_dim = input_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size,hidden_dim)\n",
    "        self.hidden2out = nn.Linear(hidden_dim,1)\n",
    "        self.hidden = self.init_hidden()\n",
    " \n",
    "    def init_hidden(self):\n",
    "        return (Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    " \n",
    "    def forward(self,seq):\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            seq.view(len(seq), 1, -1), self.hidden)\n",
    "        outdat = self.hidden2out(lstm_out.view(len(seq),-1))\n",
    "        return outdat\n",
    " \n",
    " \n",
    "model = LSTMpred(1,6)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    " \n",
    "for epoch in range(10):#设置训练次数\n",
    "    #print(epoch)\n",
    "    for seq, outs in dat[:700]:\n",
    "        seq = ToVariable(seq)\n",
    "        outs = ToVariable(outs)\n",
    "        #outs = torch.from_numpy(np.array([outs]))\n",
    " \n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        model.hidden = model.init_hidden()\n",
    " \n",
    "        modout = model(seq)\n",
    " \n",
    "        loss = loss_function(modout, outs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "predDat = []\n",
    "for seq, trueVal in dat[700:]:\n",
    "    seq = ToVariable(seq)\n",
    "    trueVal = ToVariable(trueVal)\n",
    "    predDat.append(model(seq)[-1].data.numpy()[0])\n",
    " \n",
    "fig = plt.figure()\n",
    "plt.plot(y.numpy())\n",
    "plt.plot(list(range(700,889)),predDat)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionNeuralNetwork(torch.nn.Module): # torch.nn.Module net的主模块\n",
    "    def __init__(self,n_features,n_hidden,n_output):\n",
    "        '''\n",
    "        n_features 数据个数\n",
    "        n_hidden 隐藏层的神经元的个数\n",
    "        n_output\n",
    "        '''\n",
    "        super(RegressionNeuralNetwork,self).__init__() #要继承模块的信息\n",
    "        self.hidden = torch.nn.Linear(n_features,n_hidden) # 隐藏层，输入的内容是他有多少哥输入和输出\n",
    "        self.predict = torch.nn.Linear(n_hidden,n_output) # 输出预测层，输入的内容是\n",
    "\n",
    "    def forward(self,x):\n",
    "        '''前向传播'''\n",
    "        x = F.relu(self.hidden(x)) #过一遍hidden传入上面定义的线性神经网络处理的函数中\n",
    "        x = self.predict(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([200, 1, 7]),\n torch.Size([200]),\n torch.Size([397, 1, 7]),\n torch.Size([397]))"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "tensor_data = torch.FloatTensor(df.to_numpy())[1:]\n",
    "tensor_data_y = tensor_data[:,-1]\n",
    "tensor_data_x = tensor_data[:,:-1][:,None]\n",
    "n = 200\n",
    "train_x , train_y , test_x , test_y = tensor_data_x[:n],tensor_data_y[:n],tensor_data_x[n:],tensor_data_y[n:]\n",
    "nn_features = train_x.shape[-1]\n",
    "nn_hidden = 10\n",
    "nn_output = nn_features\n",
    "train_x.shape , train_y.shape , test_x.shape , test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7) must match the size of tensor b (900) at non-singleton dimension 2",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-3cd28d80c97c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m'''计算y值的误差,传入预测值和y的实际值'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;34m'''梯度下降的终点设置为0'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2213\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2216\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (7) must match the size of tensor b (900) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "net = RegressionNeuralNetwork(n_features=nn_features,n_hidden=nn_hidden,n_output=nn_output)\n",
    "'''\n",
    "optimizer 设置优化器\n",
    "lr是学习率 这里设置为0.005\n",
    "'''\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=0.005)\n",
    "'''\n",
    "设置损失函数\n",
    "MSELoss 平方误差和做回归的损失函数比较合适\n",
    "'''\n",
    "loss_func = torch.nn.MSELoss()\n",
    "'''训练神经网络10000次'''\n",
    "for t in range(10000):\n",
    "    '''调用图神经网络预测，输入x'''\n",
    "    prediction = net(test_x) \n",
    "    '''计算y值的误差,传入预测值和y的实际值'''\n",
    "    loss = loss_func(prediction,y)\n",
    "    '''梯度下降的终点设置为0'''\n",
    "    optimizer.zero_grad() #\n",
    "    '''设置反向传播'''\n",
    "    loss.backward() #\n",
    "    ''''''\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pycharm-66c53067",
   "display_name": "PyCharm (InferenceSystem)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}