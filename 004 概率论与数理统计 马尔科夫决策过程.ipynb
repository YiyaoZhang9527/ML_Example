{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 当给定状态转移概率时，从某个状态出发存在多条马尔科夫链。对于游戏或者机器人，马尔科夫过程不足以描述其特点，因为不管是游戏还是机器人，他们都是通过动作与环境进行交互，并从环境中获得奖励，而马尔科夫过程中不存在动作和奖励。将动作（策略）和回报考虑在内的马尔科夫过程称为马尔科夫决策过程。\n",
    "### 而马尔科夫决策过程由元组 \n",
    "## $$ {(S，A，P，R，γ)}$$\n",
    "描述，其中：\n",
    "### S 为有限的状态集 \n",
    "### A 为有限的动作集 \n",
    "### P 为状态转移概率 \n",
    "### R 为回报函数 \n",
    "### γ 为折扣因子，用来计算累积回报。 \n",
    "### 注意，跟马尔科夫过程不同的是，马尔科夫决策过程的状态转移概率是包含动作的，即 。 举个例子如图2.3所示。\n",
    "<img src = \"/Users/datamagic_macbook12/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/im/Pasted Graphic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意，跟马尔科夫过程不同的是，马尔科夫决策过程的状态转移概率是包含动作的，即\n",
    "$$ P{^a}_{ss'} = P[S_{t}+1 = s'| S_t = s ,A_t = a] $$\n",
    "<img src=\"/Users/datamagic_macbook12/Library/Mobile Documents/com~apple~CloudDocs/MyProject/InferenceSystem/src/I5_algorithm/im/Pasted Graphic 2.png\">\n",
    "### 图2.3为马尔科夫决策过程的示例图，图2.3与图2.2对应。在图2.3中，学生有五个状态，\n",
    "\n",
    "## $$ 状态集为 S={s1 ，s 2 ，s 3 ，s 4 ，s 5} $$\n",
    "## $$ 动作集为A={玩，退出，学习，发表，睡觉}，$$\n",
    "在图2.3中立即回报用R标记。 强化学习的目标是给定一个马尔科夫决策过程，寻找最优策略。所谓策略是指状态到动作的映射，策略常用符号π表示，它是指给定状态s时，动作集上的一个分布，即\n",
    "\n",
    "## $$ \\pi(a|s) = P[A_t = a | S_t = s] $$\n",
    "## $ \\pi(a|s) = 某一状态下动作发生的概率 $\n",
    "## $ A_t = t时刻状态集的状态 $ \n",
    "## $ a = 状态集里 某一个状态的概率 $\n",
    "## $ S_t = t时刻动作集里的动作 $\n",
    "## $ s = 动作集里 某一时刻动作决策的概率 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}